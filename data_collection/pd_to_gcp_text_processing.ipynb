{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from pandas.io.json import json_normalize\n",
    "import re\n",
    "import random\n",
    "import time as t\n",
    "from google.cloud import storage\n",
    "from bs4 import BeautifulSoup\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main PD to GCP Command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes the content and transfers to GCP \n",
    "# requires a json with api key\n",
    "api_key_path = 'covid_news_scrape_gcp_key.json'\n",
    "bucket = 'fan_project_news_stories'\n",
    "counter = 0\n",
    "nos = 0\n",
    "folder = 'news_dfs_3.1_3.31'\n",
    "for item in os.listdir(folder):\n",
    "    news_df = pd.read_csv('{}/{}'.format(folder, item), index_col = 0, encoding='latin-1')\n",
    "    if not news_df.empty and item not in domains_used:\n",
    "        domain = news_df['source'][0].lower()\n",
    "        if '.com' in domain or '.net' in domain:\n",
    "            domain = re.sub(\"(.com)|(.net)\", \"\", domain)\n",
    "        domain = re.sub(\" \", \"_\", domain)    \n",
    "\n",
    "        def upload_text(gcp_api_key_path, bucket_name, filename, text):\n",
    "            storage_client = storage.Client.from_service_account_json(gcp_api_key_path)\n",
    "            bucket = storage_client.get_bucket(bucket_name)\n",
    "            pathname = '{domain}/{filename}.txt'.format(domain = domain, filename = filename)\n",
    "            if not bucket.blob(pathname).exists(storage_client):\n",
    "                d = bucket.blob(pathname)\n",
    "                d.upload_from_string(text)\n",
    "            else:\n",
    "                print('{} already exists in gcp; skipping'.format(filename))\n",
    "                return\n",
    "         \n",
    "        # removes any video based links\n",
    "        news_df_no_vids = news_df[~news_df['url'].str.contains('/video')]\n",
    "        news_df_no_vids = news_df_no_vids[~news_df_no_vids['url'].str.contains('/radio')]\n",
    "        news_df_no_vids = news_df_no_vids[~news_df_no_vids['url'].str.contains('/audio')]\n",
    "        \n",
    "        #print(\"{} na rate: {}\".format(domain, news_df_no_vids['content'].isna().mean()))\n",
    "        for index, row in news_df.iterrows():\n",
    "            # construct the datetime metadata\n",
    "            date = re.sub( '-', '.', row['publishedAt'].split(\"T\",1)[0])\n",
    "            time = re.sub(\":\", '.', row['publishedAt'].split(\"T\",1)[1].split(\"Z\",1)[0])\n",
    "\n",
    "            source = re.sub(\"(.com)|(.net)\", \"\", row['source'])\n",
    "            metadata = '{}_{}.{}'.format(source, date, time)\n",
    "            full_text = row['content']\n",
    "            if type(full_text) is str and len(full_text) > 100:\n",
    "                print('index: {} uploading document {} from df\\n'.format(index, metadata))\n",
    "                upload_text(api_key_path, bucket, '{metadata}.txt'.format(metadata=metadata), full_text)\n",
    "            else:\n",
    "                nos += 1\n",
    "                print('index: {} document {} was empty and was skipped\\n'.format(index, metadata))\n",
    "            counter += 1\n",
    "            t.sleep(random.uniform(0,1))\n",
    "    domains_used.append(item)\n",
    "print(\"total: {}\\nNaNs: {}\\nNaN Rate: {}\".format(counter, nos, nos/counter) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Various other scraping cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN Scraping\n",
    "# Imports the Google Cloud client library\n",
    "from google.cloud import storage\n",
    "%env GOOGLE_APPLICATION_CREDENTIALS=\"/Desktop/Coding/NewsScrape/covid_news_scrape_gcp_key.json\"\n",
    "news_df = pd.read_csv('{}/news_cnn.com_11.01_11.15.csv'.format(folder), index_col = 0)\n",
    "news_df = news_df[news_df['url'].notnull()]\n",
    "news_df_no_vids = news_df[~news_df['url'].str.contains('video')]\n",
    "news_df_no_vids = news_df_no_vids[~news_df_no_vids['url'].str.contains('rss')]\n",
    "\n",
    "def has_paragraph(css_class):\n",
    "    return css_class is not None and 'paragraph' in css_class\n",
    "\n",
    "def upload_text(gcp_api_key_path, bucket_name, filename, text):\n",
    "    storage_client = storage.Client.from_service_account_json(gcp_api_key_path)\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "    pathname = 'cnn/{}.txt'.format(filename)\n",
    "    d = bucket.blob(pathname)\n",
    "    d.upload_from_string(text)\n",
    "\n",
    "api_key_path = 'covid_news_scrape_gcp_key.json'\n",
    "bucket = 'fan_project_news_stories'\n",
    "for index, row in news_df_no_vids.iterrows():\n",
    "    try:\n",
    "        # construct the datetime metadata\n",
    "        date = re.sub( '-', '.', row['publishedAt'].split(\"T\",1)[0])\n",
    "        time = re.sub(\":\", '.', row['publishedAt'].split(\"T\",1)[1].split(\"Z\",1)[0])\n",
    "\n",
    "        source = row['source']\n",
    "        metadata = '{}_{}.{}'.format(source, date, time)\n",
    "\n",
    "        # get the text\n",
    "        resp = requests.get(row['url'])\n",
    "        news_html = resp.text\n",
    "        document = BeautifulSoup(news_html, 'html.parser')\n",
    "\n",
    "        tags = document.find_all(class_=has_paragraph) # find 'div' tags that have attribute class = referent\n",
    "        text_list = []\n",
    "        for tag in tags:\n",
    "            if tag.get_text() not in text_list:\n",
    "                text_list.append(tag.get_text()) \n",
    "\n",
    "        full_text = ' '.join(text_list)\n",
    "\n",
    "        if '(CNN)' in full_text:\n",
    "            full_text = full_text.split('(CNN)', 1)[1]\n",
    "\n",
    "        if len(full_text) > 0:\n",
    "            print('uploading document {} from df'.format(metadata))\n",
    "            upload_text(api_key_path, bucket, '{metadata}.txt'.format(metadata=metadata), full_text)\n",
    "        else:\n",
    "            print('document {} was empty and was skipped'.format(metadata))\n",
    "        t.sleep(random.uniform(1,3))\n",
    "    except:\n",
    "        print('error in document {} and skipping'.format(metadata))\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fox News Scraping\n",
    "\n",
    "from google.cloud import storage\n",
    "%env GOOGLE_APPLICATION_CREDENTIALS=\"/Desktop/Coding/NewsScrape/covid_news_scrape_gcp_key.json\"\n",
    "news_df = pd.read_csv('{}/news_foxnews.com_11.01_11.15.csv'.format(folder), index_col = 0)\n",
    "news_df = news_df[news_df['url'].notnull()]\n",
    "news_df_no_vids = news_df[~news_df['url'].str.contains('video')]\n",
    "news_df_no_vids = news_df_no_vids[~news_df_no_vids['url'].str.contains('radio')]\n",
    "\n",
    "def upload_text(gcp_api_key_path, bucket_name, filename, text):\n",
    "    storage_client = storage.Client.from_service_account_json(gcp_api_key_path)\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "    pathname = 'foxnews/{}.txt'.format(filename)\n",
    "    d = bucket.blob(pathname)\n",
    "    d.upload_from_string(text)\n",
    "    \n",
    "api_key_path = 'covid_news_scrape_gcp_key.json'\n",
    "bucket = 'fan_project_news_stories'\n",
    "for index, row in news_df_no_vids.iterrows():\n",
    "    try:\n",
    "        # construct the datetime metadata\n",
    "        date = re.sub( '-', '.', row['publishedAt'].split(\"T\",1)[0])\n",
    "        time = re.sub(\":\", '.', row['publishedAt'].split(\"T\",1)[1].split(\"Z\",1)[0])\n",
    "\n",
    "        source = row['source']\n",
    "        metadata = '{}_{}.{}'.format(source, date, time)\n",
    "\n",
    "        # get the text\n",
    "\n",
    "        resp = requests.get(row['url'])\n",
    "        news_html = resp.text\n",
    "        document = BeautifulSoup(news_html, 'html.parser')\n",
    "\n",
    "        div = document.find(\"div\", {\"class\": \"article-body\"})\n",
    "\n",
    "\n",
    "        p_tags = div.findAll('p')\n",
    "\n",
    "\n",
    "        text_list = []\n",
    "        for tag in p_tags:\n",
    "            if 'Foxnews.com' not in tag.get_text() and tag.get_text() not in text_list:\n",
    "                text_list.append(tag.get_text())\n",
    "\n",
    "\n",
    "        full_text = ' '.join(text_list)\n",
    "\n",
    "        if len(full_text) > 0:\n",
    "            print('uploading document {} from df'.format(metadata))\n",
    "            upload_text(api_key_path, bucket, '{metadata}.txt'.format(metadata=metadata), full_text)\n",
    "        else:\n",
    "            print('document {} was empty and was skipped'.format(metadata))\n",
    "        t.sleep(random.uniform(1,3))\n",
    "    except:\n",
    "        print('error in document {} and skipping'.format(metadata))\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NBC News Scraping\n",
    "from google.cloud import storage\n",
    "%env GOOGLE_APPLICATION_CREDENTIALS=\"/Desktop/Coding/NewsScrape/covid_news_scrape_gcp_key.json\"\n",
    "\n",
    "news_df = pd.read_csv('{}/news_nbcnews.com_11.01_11.15.csv'.format(folder), index_col = 0)\n",
    "news_df = news_df[news_df['url'].notnull()]\n",
    "news_df_no_vids = news_df[~news_df['url'].str.contains('video')]\n",
    "news_df_no_vids = news_df_no_vids[~news_df_no_vids['url'].str.contains('radio')]\n",
    "\n",
    "def upload_text(gcp_api_key_path, bucket_name, filename, text):\n",
    "    storage_client = storage.Client.from_service_account_json(gcp_api_key_path)\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "    pathname = 'nbcnews/{}.txt'.format(filename)\n",
    "    d = bucket.blob(pathname)\n",
    "    d.upload_from_string(text)\n",
    "    \n",
    "api_key_path = 'covid_news_scrape_gcp_key.json'\n",
    "bucket = 'fan_project_news_stories'\n",
    "for index, row in news_df_no_vids.iterrows():\n",
    "    try:\n",
    "\n",
    "        # construct the datetime metadata\n",
    "        date = re.sub( '-', '.', row['publishedAt'].split(\"T\",1)[0])\n",
    "        time = re.sub(\":\", '.', row['publishedAt'].split(\"T\",1)[1].split(\"Z\",1)[0])\n",
    "\n",
    "        source = row['source']\n",
    "        metadata = '{}_{}.{}'.format(source, date, time)\n",
    "\n",
    "        # get the text\n",
    "        resp = requests.get(row['url'])\n",
    "        news_html = resp.text\n",
    "        document = BeautifulSoup(news_html, 'html.parser')\n",
    "\n",
    "        div = document.find(\"div\", {\"class\": \"article-body__content\"})\n",
    "\n",
    "\n",
    "        p_tags = div.findAll('p')\n",
    "\n",
    "        text_list = []\n",
    "        for tag in p_tags:\n",
    "            if 'Privacy Policy' not in tag.get_text() and tag.get_text() not in text_list:\n",
    "                text_list.append(tag.get_text())\n",
    "\n",
    "\n",
    "        full_text = ' '.join(text_list)\n",
    "\n",
    "        if len(full_text) > 0:\n",
    "            print('uploading document {} from df'.format(metadata))\n",
    "            upload_text(api_key_path, bucket, '{metadata}.txt'.format(metadata=metadata), full_text)\n",
    "        else:\n",
    "            print('document {} was empty and was skipped'.format(metadata))\n",
    "        t.sleep(random.uniform(1,3))\n",
    "    except:\n",
    "        print(\"error, skipping document {}\".format(index))\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AJC scraping\n",
    "\n",
    "\n",
    "from google.cloud import storage\n",
    "%env GOOGLE_APPLICATION_CREDENTIALS=\"/Desktop/Coding/NewsScrape/covid_news_scrape_gcp_key.json\"\n",
    "\n",
    "def upload_text(gcp_api_key_path, bucket_name, filename, text):\n",
    "    storage_client = storage.Client.from_service_account_json(gcp_api_key_path)\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "    pathname = 'ajc/{}.txt'.format(filename)\n",
    "    d = bucket.blob(pathname)\n",
    "    d.upload_from_string(text)\n",
    "\n",
    "    \n",
    "news_df = pd.read_csv('{}/news_ajc.com_11.01_11.15.csv'.format(folder), index_col = 0)\n",
    "news_df = news_df[news_df['url'].notnull()]\n",
    "\n",
    "api_key_path = 'covid_news_scrape_gcp_key.json'\n",
    "bucket = 'fan_project_news_stories'\n",
    "for index, row in news_df.iterrows():\n",
    "    try:\n",
    "        # construct the datetime metadata\n",
    "        date = re.sub( '-', '.', row['publishedAt'].split(\"T\",1)[0])\n",
    "        time = re.sub(\":\", '.', row['publishedAt'].split(\"T\",1)[1].split(\"Z\",1)[0])\n",
    "\n",
    "        source = row['source']\n",
    "        metadata = '{}_{}.{}'.format(source, date, time)\n",
    "\n",
    "        # get the text\n",
    "\n",
    "        resp = requests.get(row['url'])\n",
    "        news_html = resp.text\n",
    "        document = BeautifulSoup(news_html, 'html.parser')\n",
    "\n",
    "        divs = document.findAll('div', {'class': 'c-section'})\n",
    "\n",
    "        text_list = []\n",
    "        for div in divs:\n",
    "            if div.get_text() not in text_list:\n",
    "                text_list.append(div.get_text())\n",
    "\n",
    "\n",
    "        full_text = ' '.join(text_list)\n",
    "\n",
    "        if len(full_text) > 0:\n",
    "            print('uploading document {} from df'.format(metadata))\n",
    "            upload_text(api_key_path, bucket, '{metadata}.txt'.format(metadata=metadata), full_text)\n",
    "        else:\n",
    "            print('document {} was empty and was skipped'.format(metadata))\n",
    "        t.sleep(random.uniform(1,3))\n",
    "    except:\n",
    "        print('error in document {} and skipping'.format(metadata))\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# azcentral scraping\n",
    "\n",
    "\n",
    "from google.cloud import storage\n",
    "%env GOOGLE_APPLICATION_CREDENTIALS=\"/Desktop/Coding/NewsScrape/covid_news_scrape_gcp_key.json\"\n",
    "news_df = pd.read_csv('{}/news_azcentral.com_11.01_11.15.csv'.format(folder), index_col = 0)\n",
    "news_df = news_df[news_df['url'].notnull()]\n",
    "news_df_no_vids = news_df[~news_df['url'].str.contains('clip')]\n",
    "\n",
    "\n",
    "def upload_text(gcp_api_key_path, bucket_name, filename, text):\n",
    "    storage_client = storage.Client.from_service_account_json(gcp_api_key_path)\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "    pathname = 'azcentral/{}.txt'.format(filename)\n",
    "    d = bucket.blob(pathname)\n",
    "    d.upload_from_string(text)\n",
    "    \n",
    "api_key_path = 'covid_news_scrape_gcp_key.json'\n",
    "bucket = 'fan_project_news_stories'\n",
    "for index, row in news_df.iterrows():\n",
    "    try:\n",
    "        # construct the datetime metadata\n",
    "        date = re.sub( '-', '.', row['publishedAt'].split(\"T\",1)[0])\n",
    "        time = re.sub(\":\", '.', row['publishedAt'].split(\"T\",1)[1].split(\"Z\",1)[0])\n",
    "\n",
    "        source = row['source']\n",
    "        metadata = '{}_{}.{}'.format(source, date, time)\n",
    "\n",
    "        # get the text\n",
    "\n",
    "        resp = requests.get(row['url'])\n",
    "        news_html = resp.text\n",
    "        document = BeautifulSoup(news_html, 'html.parser')\n",
    "\n",
    "        divs = document.findAll('p', {'class': 'gnt_ar_b_p'})\n",
    "\n",
    "        text_list = []\n",
    "        for div in divs:\n",
    "            if div.select('p > strong') or div.select('p > em'):\n",
    "                continue\n",
    "            if div.get_text() not in text_list:\n",
    "                text_list.append(div.get_text())\n",
    "\n",
    "\n",
    "        full_text = ' '.join(text_list)\n",
    "\n",
    "        if len(full_text) > 0:\n",
    "            print('uploading document {} from df'.format(metadata))\n",
    "            upload_text(api_key_path, bucket, '{metadata}.txt'.format(metadata=metadata), full_text)\n",
    "        else:\n",
    "            print('document {} was empty and was skipped'.format(metadata))\n",
    "        t.sleep(random.uniform(1,3))\n",
    "    except:\n",
    "        print('error in document {} and skipping'.format(metadata))\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# breitbart scraping\n",
    "from google.cloud import storage\n",
    "%env GOOGLE_APPLICATION_CREDENTIALS=\"/Desktop/Coding/NewsScrape/covid_news_scrape_gcp_key.json\"\n",
    "\n",
    "def upload_text(gcp_api_key_path, bucket_name, filename, text):\n",
    "    storage_client = storage.Client.from_service_account_json(gcp_api_key_path)\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "    pathname = 'breitbart/{}.txt'.format(filename)\n",
    "    d = bucket.blob(pathname)\n",
    "    d.upload_from_string(text)\n",
    "    \n",
    "news_df = pd.read_csv('{}/news_breitbart.com_11.01_11.15.csv'.format(folder), index_col = 0)\n",
    "news_df = news_df[news_df['url'].notnull()]\n",
    "news_df_no_vids = news_df[~news_df['url'].str.contains('clip')]\n",
    "\n",
    "api_key_path = 'covid_news_scrape_gcp_key.json'\n",
    "bucket = 'fan_project_news_stories'\n",
    "for index, row in news_df.iterrows():\n",
    "    try:\n",
    "        # construct the datetime metadata\n",
    "        date = re.sub( '-', '.', row['publishedAt'].split(\"T\",1)[0])\n",
    "        time = re.sub(\":\", '.', row['publishedAt'].split(\"T\",1)[1].split(\"Z\",1)[0])\n",
    "\n",
    "        source = row['source']\n",
    "        metadata = '{}_{}.{}'.format(source, date, time)\n",
    "\n",
    "        # get the text\n",
    "\n",
    "        resp = requests.get(row['url'])\n",
    "        news_html = resp.text\n",
    "        document = BeautifulSoup(news_html, 'html.parser')\n",
    "        \n",
    "        main_div = document.find('div', {'class': 'entry-content'})\n",
    "        divs = main_div.findAll('p')\n",
    "\n",
    "        text_list = []\n",
    "        for div in divs:\n",
    "            if div.select('p > strong') or div.select('p > em') or \"Follow @\" in div.get_text():\n",
    "                continue\n",
    "            if div.get_text() not in text_list:\n",
    "                text_list.append(div.get_text())\n",
    "\n",
    "\n",
    "        full_text = ' '.join(text_list)\n",
    "\n",
    "        if len(full_text) > 0:\n",
    "            print('uploading document {} from df'.format(metadata))\n",
    "            upload_text(api_key_path, bucket, '{metadata}.txt'.format(metadata=metadata), full_text)\n",
    "        else:\n",
    "            print('document {} was empty and was skipped'.format(metadata))\n",
    "        t.sleep(random.uniform(1,3))\n",
    "    except:\n",
    "        print('error in document {} and skipping'.format(metadata))\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#USA TODAY Scraping\n",
    "from google.cloud import storage\n",
    "%env GOOGLE_APPLICATION_CREDENTIALS=\"/Desktop/Coding/NewsScrape/covid_news_scrape_gcp_key.json\"\n",
    "def upload_text(gcp_api_key_path, bucket_name, filename, text):\n",
    "    storage_client = storage.Client.from_service_account_json(gcp_api_key_path)\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "    pathname = 'usatoday/{}.txt'.format(filename)\n",
    "    d = bucket.blob(pathname)\n",
    "    d.upload_from_string(text)\n",
    "    \n",
    "    \n",
    "news_df = pd.read_csv('{}/news_usatoday.com_11.01_11.15.csv'.format(folder), index_col = 0)\n",
    "news_df = news_df[news_df['url'].notnull()]\n",
    "news_df = news_df[~news_df['url'].str.contains('mmajunkie')]\n",
    "api_key_path = 'covid_news_scrape_gcp_key.json'\n",
    "bucket = 'fan_project_news_stories'\n",
    "for index, row in news_df.iterrows():\n",
    "    try:\n",
    "        # construct the datetime metadata\n",
    "        date = re.sub( '-', '.', row['publishedAt'].split(\"T\",1)[0])\n",
    "        time = re.sub(\":\", '.', row['publishedAt'].split(\"T\",1)[1].split(\"Z\",1)[0])\n",
    "\n",
    "        source = row['source']\n",
    "        metadata = '{}_{}.{}'.format(source, date, time)\n",
    "\n",
    "        # get the text\n",
    "\n",
    "        resp = requests.get(row['url'])\n",
    "        news_html = resp.text\n",
    "        document = BeautifulSoup(news_html, 'html.parser')\n",
    "        document\n",
    "\n",
    "        divs = document.findAll('p', {'class': 'gnt_ar_b_p'})\n",
    "\n",
    "        text_list = []\n",
    "        for div in divs:\n",
    "            if div.select('p > strong') or div.select('p > em'):\n",
    "                continue\n",
    "            if div.get_text() not in text_list:\n",
    "                text_list.append(div.get_text())\n",
    "\n",
    "\n",
    "        full_text = ' '.join(text_list)\n",
    "\n",
    "        if len(full_text) > 0:\n",
    "            print('uploading document {} from df'.format(metadata))\n",
    "            upload_text(api_key_path, bucket, '{metadata}.txt'.format(metadata=metadata), full_text)\n",
    "        else:\n",
    "            print('document {} was empty and was skipped'.format(metadata))\n",
    "        t.sleep(random.uniform(1,3))\n",
    "    except:\n",
    "        print('error in document {} and skipping'.format(metadata))\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NY POST\n",
    "from google.cloud import storage\n",
    "news_df = pd.read_csv('{}/news_nypost.com_11.01_11.15.csv'.format(folder), index_col = 0)\n",
    "news_df = news_df[news_df['url'].notnull()]\n",
    "\n",
    "\n",
    "%env GOOGLE_APPLICATION_CREDENTIALS=\"/Desktop/Coding/NewsScrape/covid_news_scrape_gcp_key.json\"\n",
    "def upload_text(gcp_api_key_path, bucket_name, filename, text):\n",
    "    storage_client = storage.Client.from_service_account_json(gcp_api_key_path)\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "    pathname = 'nypost/{}.txt'.format(filename)\n",
    "    d = bucket.blob(pathname)\n",
    "    d.upload_from_string(text)\n",
    "    \n",
    "api_key_path = 'covid_news_scrape_gcp_key.json'\n",
    "bucket = 'fan_project_news_stories'\n",
    "for index, row in news_df.iterrows():\n",
    "    try:\n",
    "        # construct the datetime metadata\n",
    "        date = re.sub( '-', '.', row['publishedAt'].split(\"T\",1)[0])\n",
    "        time = re.sub(\":\", '.', row['publishedAt'].split(\"T\",1)[1].split(\"Z\",1)[0])\n",
    "\n",
    "        source = row['source']\n",
    "        metadata = '{}_{}.{}'.format(source, date, time)\n",
    "\n",
    "        # get the text\n",
    "\n",
    "        resp = requests.get(row['url'])\n",
    "        news_html = resp.text\n",
    "        document = BeautifulSoup(news_html, 'html.parser')\n",
    "\n",
    "        document = document.find('div', {'class': 'entry-content entry-content-read-more'})\n",
    "\n",
    "        divs = document.findAll('p')\n",
    "\n",
    "        text_list = []\n",
    "        for div in divs:\n",
    "            if div.select('p > strong') or div.select('p > em'):\n",
    "                continue\n",
    "            if div.get_text() not in text_list:\n",
    "                text_list.append(div.get_text())\n",
    "\n",
    "        full_text = ' '.join(text_list)\n",
    "\n",
    "        if len(full_text) > 0:\n",
    "            print('uploading document {} from df'.format(metadata))\n",
    "            upload_text(api_key_path, bucket, '{metadata}.txt'.format(metadata=metadata), full_text)\n",
    "        else:\n",
    "            print('document {} was empty and was skipped'.format(metadata))\n",
    "        t.sleep(random.uniform(1,3))\n",
    "    except:\n",
    "        print('error in document {} and skipping'.format(metadata))\n",
    "        continue"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
