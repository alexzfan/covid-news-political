{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexfan/anaconda3/lib/python3.7/site-packages/past/types/oldstr.py:5: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import Iterable\n"
     ]
    }
   ],
   "source": [
    "import logging \n",
    "import itertools \n",
    "import numpy as np \n",
    "import gensim \n",
    "\n",
    "# import some more modules for processing the corpus\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "import os \n",
    "\n",
    "# configure logging \n",
    "logging.basicConfig(format='%(levelname)s : %(message)s', level=logging.INFO)\n",
    "logging.root.level = logging.INFO  \n",
    "\n",
    "def head(stream, n=10):\n",
    "    return list(itertools.islice(stream, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize\n",
    "def tokenize(text):\n",
    "    return [token for token in simple_preprocess(text) if token not in STOPWORDS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_docs(base_dir):\n",
    "    docCount = 0\n",
    "    docs = os.listdir(base_dir)\n",
    "\n",
    "    for doc in docs:\n",
    "        if not doc.startswith('.'):\n",
    "            with open(base_dir + doc, \"r\") as file:\n",
    "                text = file.read()\n",
    "                tokens = tokenize(text) \n",
    "        \n",
    "                yield doc, tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = iter_docs('./test/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO : built Dictionary(4236 unique tokens: ['able', 'accessory', 'accommodate', 'adequate', 'adopt']...) from 106 documents (total 64576 corpus positions)\n"
     ]
    }
   ],
   "source": [
    "doc_stream = (tokens for _, tokens in iter_docs('./test/'))\n",
    "              \n",
    "id2word = gensim.corpora.Dictionary(doc_stream) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : discarding 0 tokens: []...\n",
      "INFO : keeping 4236 tokens which were in no less than 2 and no more than 106 (=100.0%) documents\n",
      "INFO : resulting dictionary: Dictionary(4236 unique tokens: ['able', 'accessory', 'accommodate', 'adequate', 'adopt']...)\n"
     ]
    }
   ],
   "source": [
    "# filter out words in only 1 doc, keeping the rest\n",
    "id2word.filter_extremes(no_below=2, no_above=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Corpus(object):\n",
    "    def __init__(self, dump_file, dictionary, clip_docs=None):\n",
    "        self.dump_file = dump_file\n",
    "        self.dictionary = dictionary\n",
    "        self.clip_docs = clip_docs\n",
    "    \n",
    "    def __iter__(self):\n",
    "        self.titles = []\n",
    "        for title, tokens in itertools.islice(iter_docs(self.dump_file), self.clip_docs):\n",
    "            self.titles.append(title)\n",
    "            yield self.dictionary.doc2bow(tokens)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.clip_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_corpus = Corpus('./test/', id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : using symmetric alpha at 0.05\n",
      "INFO : using symmetric eta at 0.05\n",
      "INFO : using serial LDA version on this node\n",
      "WARNING : input corpus stream has no len(); counting documents\n",
      "INFO : running online (multi-pass) LDA training, 20 topics, 10 passes over the supplied corpus of 106 documents, updating model once every 106 documents, evaluating perplexity every 106 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "INFO : -11.142 per-word bound, 2259.8 perplexity estimate based on a held-out corpus of 106 documents with 64576 words\n",
      "INFO : PROGRESS: pass 0, at document #106/106\n",
      "INFO : topic #14 (0.050): 0.008*\"hong\" + 0.007*\"kong\" + 0.006*\"people\" + 0.006*\"said\" + 0.006*\"food\" + 0.005*\"fiftyforward\" + 0.005*\"china\" + 0.005*\"roy\" + 0.005*\"li\" + 0.004*\"life\"\n",
      "INFO : topic #10 (0.050): 0.022*\"li\" + 0.018*\"school\" + 0.010*\"nashville\" + 0.009*\"said\" + 0.007*\"tn\" + 0.006*\"elementary\" + 0.006*\"food\" + 0.005*\"meals\" + 0.004*\"middle\" + 0.004*\"kong\"\n",
      "INFO : topic #3 (0.050): 0.009*\"says\" + 0.006*\"people\" + 0.005*\"office\" + 0.005*\"work\" + 0.005*\"school\" + 0.005*\"kong\" + 0.005*\"china\" + 0.004*\"new\" + 0.004*\"li\" + 0.004*\"hong\"\n",
      "INFO : topic #2 (0.050): 0.006*\"people\" + 0.006*\"says\" + 0.005*\"food\" + 0.004*\"day\" + 0.004*\"new\" + 0.004*\"fiftyforward\" + 0.004*\"office\" + 0.004*\"members\" + 0.003*\"employees\" + 0.003*\"work\"\n",
      "INFO : topic #4 (0.050): 0.015*\"li\" + 0.010*\"said\" + 0.009*\"jones\" + 0.008*\"judge\" + 0.006*\"school\" + 0.005*\"drive\" + 0.004*\"like\" + 0.004*\"going\" + 0.004*\"middle\" + 0.003*\"china\"\n",
      "INFO : topic diff=3.626551, rho=1.000000\n",
      "INFO : -8.724 per-word bound, 422.9 perplexity estimate based on a held-out corpus of 106 documents with 64576 words\n",
      "INFO : PROGRESS: pass 1, at document #106/106\n",
      "INFO : topic #17 (0.050): 0.021*\"li\" + 0.006*\"school\" + 0.006*\"hotel\" + 0.005*\"middle\" + 0.005*\"year\" + 0.005*\"says\" + 0.004*\"travel\" + 0.004*\"said\" + 0.004*\"nashville\" + 0.004*\"covid\"\n",
      "INFO : topic #16 (0.050): 0.019*\"says\" + 0.008*\"people\" + 0.007*\"company\" + 0.007*\"boston\" + 0.006*\"li\" + 0.005*\"school\" + 0.005*\"work\" + 0.004*\"care\" + 0.004*\"home\" + 0.004*\"services\"\n",
      "INFO : topic #13 (0.050): 0.007*\"testing\" + 0.005*\"mail\" + 0.005*\"people\" + 0.005*\"new\" + 0.004*\"obama\" + 0.004*\"qanon\" + 0.004*\"week\" + 0.004*\"mr\" + 0.004*\"state\" + 0.004*\"trump\"\n",
      "INFO : topic #4 (0.050): 0.018*\"jones\" + 0.016*\"judge\" + 0.014*\"said\" + 0.009*\"li\" + 0.007*\"red\" + 0.007*\"like\" + 0.006*\"going\" + 0.006*\"need\" + 0.006*\"bit\" + 0.006*\"guy\"\n",
      "INFO : topic #9 (0.050): 0.018*\"company\" + 0.016*\"says\" + 0.015*\"cohen\" + 0.011*\"mallett\" + 0.009*\"people\" + 0.009*\"day\" + 0.008*\"ezcater\" + 0.007*\"layoffs\" + 0.007*\"zoom\" + 0.007*\"work\"\n",
      "INFO : topic diff=2.354710, rho=0.577350\n",
      "INFO : -7.540 per-word bound, 186.1 perplexity estimate based on a held-out corpus of 106 documents with 64576 words\n",
      "INFO : PROGRESS: pass 2, at document #106/106\n",
      "INFO : topic #11 (0.050): 0.089*\"li\" + 0.042*\"school\" + 0.023*\"nashville\" + 0.017*\"elementary\" + 0.017*\"middle\" + 0.015*\"tn\" + 0.012*\"food\" + 0.012*\"ul\" + 0.012*\"drive\" + 0.011*\"bus\"\n",
      "INFO : topic #19 (0.050): 0.040*\"hong\" + 0.037*\"kong\" + 0.014*\"people\" + 0.013*\"independence\" + 0.012*\"says\" + 0.011*\"beijing\" + 0.009*\"china\" + 0.009*\"city\" + 0.007*\"protests\" + 0.007*\"time\"\n",
      "INFO : topic #18 (0.050): 0.018*\"li\" + 0.011*\"school\" + 0.006*\"says\" + 0.006*\"tn\" + 0.006*\"nashville\" + 0.005*\"midsommar\" + 0.005*\"middle\" + 0.005*\"dress\" + 0.005*\"coronavirus\" + 0.005*\"elementary\"\n",
      "INFO : topic #14 (0.050): 0.015*\"roy\" + 0.015*\"fiftyforward\" + 0.013*\"food\" + 0.013*\"life\" + 0.009*\"foundation\" + 0.009*\"bordeaux\" + 0.008*\"said\" + 0.008*\"years\" + 0.007*\"hot\" + 0.007*\"members\"\n",
      "INFO : topic #3 (0.050): 0.017*\"says\" + 0.013*\"office\" + 0.009*\"work\" + 0.008*\"employees\" + 0.008*\"people\" + 0.008*\"pandemic\" + 0.007*\"care\" + 0.007*\"boston\" + 0.006*\"percent\" + 0.006*\"business\"\n",
      "INFO : topic diff=2.276627, rho=0.500000\n",
      "INFO : -7.073 per-word bound, 134.6 perplexity estimate based on a held-out corpus of 106 documents with 64576 words\n",
      "INFO : PROGRESS: pass 3, at document #106/106\n",
      "INFO : topic #7 (0.050): 0.052*\"hong\" + 0.047*\"kong\" + 0.024*\"said\" + 0.011*\"government\" + 0.010*\"china\" + 0.008*\"mainland\" + 0.008*\"immigration\" + 0.008*\"law\" + 0.007*\"kongs\" + 0.007*\"citys\"\n",
      "INFO : topic #19 (0.050): 0.042*\"hong\" + 0.039*\"kong\" + 0.015*\"people\" + 0.014*\"independence\" + 0.012*\"says\" + 0.012*\"beijing\" + 0.009*\"china\" + 0.009*\"city\" + 0.008*\"protests\" + 0.008*\"time\"\n",
      "INFO : topic #1 (0.050): 0.004*\"hong\" + 0.003*\"kong\" + 0.003*\"people\" + 0.003*\"said\" + 0.002*\"says\" + 0.002*\"work\" + 0.002*\"china\" + 0.002*\"government\" + 0.002*\"week\" + 0.002*\"like\"\n",
      "INFO : topic #3 (0.050): 0.019*\"says\" + 0.014*\"office\" + 0.010*\"work\" + 0.009*\"employees\" + 0.009*\"people\" + 0.008*\"pandemic\" + 0.008*\"care\" + 0.007*\"boston\" + 0.007*\"percent\" + 0.007*\"business\"\n",
      "INFO : topic #12 (0.050): 0.004*\"li\" + 0.003*\"people\" + 0.003*\"school\" + 0.002*\"says\" + 0.002*\"testing\" + 0.002*\"said\" + 0.002*\"new\" + 0.002*\"work\" + 0.002*\"coronavirus\" + 0.002*\"qanon\"\n",
      "INFO : topic diff=2.025660, rho=0.447214\n",
      "INFO : -6.847 per-word bound, 115.1 perplexity estimate based on a held-out corpus of 106 documents with 64576 words\n",
      "INFO : PROGRESS: pass 4, at document #106/106\n",
      "INFO : topic #13 (0.050): 0.008*\"testing\" + 0.005*\"mail\" + 0.005*\"new\" + 0.005*\"obama\" + 0.005*\"qanon\" + 0.005*\"people\" + 0.005*\"week\" + 0.005*\"mr\" + 0.004*\"state\" + 0.004*\"trump\"\n",
      "INFO : topic #0 (0.050): 0.022*\"hong\" + 0.018*\"kong\" + 0.011*\"said\" + 0.006*\"people\" + 0.006*\"says\" + 0.006*\"li\" + 0.005*\"government\" + 0.005*\"china\" + 0.004*\"mainland\" + 0.004*\"work\"\n",
      "INFO : topic #6 (0.050): 0.020*\"dress\" + 0.020*\"midsommar\" + 0.012*\"went\" + 0.012*\"queen\" + 0.012*\"auction\" + 0.012*\"academy\" + 0.012*\"worn\" + 0.012*\"museum\" + 0.012*\"sold\" + 0.012*\"items\"\n",
      "INFO : topic #4 (0.050): 0.028*\"jones\" + 0.026*\"judge\" + 0.019*\"said\" + 0.012*\"red\" + 0.010*\"like\" + 0.009*\"going\" + 0.009*\"need\" + 0.009*\"bit\" + 0.009*\"guy\" + 0.009*\"little\"\n",
      "INFO : topic #7 (0.050): 0.053*\"hong\" + 0.047*\"kong\" + 0.025*\"said\" + 0.011*\"government\" + 0.010*\"china\" + 0.009*\"mainland\" + 0.008*\"immigration\" + 0.008*\"law\" + 0.007*\"kongs\" + 0.007*\"citys\"\n",
      "INFO : topic diff=1.701141, rho=0.408248\n",
      "INFO : -6.720 per-word bound, 105.4 perplexity estimate based on a held-out corpus of 106 documents with 64576 words\n",
      "INFO : PROGRESS: pass 5, at document #106/106\n",
      "INFO : topic #6 (0.050): 0.021*\"dress\" + 0.021*\"midsommar\" + 0.013*\"went\" + 0.013*\"queen\" + 0.013*\"auction\" + 0.013*\"academy\" + 0.013*\"worn\" + 0.013*\"museum\" + 0.013*\"sold\" + 0.013*\"items\"\n",
      "INFO : topic #11 (0.050): 0.091*\"li\" + 0.044*\"school\" + 0.024*\"nashville\" + 0.017*\"elementary\" + 0.017*\"middle\" + 0.015*\"tn\" + 0.013*\"food\" + 0.013*\"ul\" + 0.013*\"drive\" + 0.011*\"bus\"\n",
      "INFO : topic #13 (0.050): 0.008*\"testing\" + 0.005*\"mail\" + 0.005*\"new\" + 0.005*\"obama\" + 0.005*\"qanon\" + 0.005*\"people\" + 0.005*\"week\" + 0.005*\"mr\" + 0.004*\"state\" + 0.004*\"trump\"\n",
      "INFO : topic #15 (0.050): 0.021*\"police\" + 0.021*\"paris\" + 0.017*\"protestors\" + 0.017*\"yellow\" + 0.013*\"jackets\" + 0.012*\"french\" + 0.012*\"showed\" + 0.012*\"movement\" + 0.012*\"protests\" + 0.008*\"government\"\n",
      "INFO : topic #9 (0.050): 0.026*\"company\" + 0.023*\"says\" + 0.021*\"cohen\" + 0.017*\"mallett\" + 0.012*\"people\" + 0.012*\"ezcater\" + 0.011*\"day\" + 0.011*\"layoffs\" + 0.011*\"zoom\" + 0.009*\"work\"\n",
      "INFO : topic diff=1.367554, rho=0.377964\n",
      "INFO : -6.645 per-word bound, 100.1 perplexity estimate based on a held-out corpus of 106 documents with 64576 words\n",
      "INFO : PROGRESS: pass 6, at document #106/106\n",
      "INFO : topic #11 (0.050): 0.091*\"li\" + 0.044*\"school\" + 0.024*\"nashville\" + 0.017*\"elementary\" + 0.017*\"middle\" + 0.015*\"tn\" + 0.013*\"food\" + 0.013*\"ul\" + 0.013*\"drive\" + 0.011*\"bus\"\n",
      "INFO : topic #14 (0.050): 0.017*\"roy\" + 0.017*\"fiftyforward\" + 0.015*\"food\" + 0.015*\"life\" + 0.011*\"foundation\" + 0.011*\"bordeaux\" + 0.009*\"said\" + 0.009*\"years\" + 0.009*\"hot\" + 0.009*\"members\"\n",
      "INFO : topic #17 (0.050): 0.012*\"hotel\" + 0.008*\"year\" + 0.008*\"travel\" + 0.008*\"ll\" + 0.008*\"caribbean\" + 0.008*\"night\" + 0.006*\"covid\" + 0.006*\"like\" + 0.006*\"new\" + 0.006*\"stay\"\n",
      "INFO : topic #4 (0.050): 0.030*\"jones\" + 0.027*\"judge\" + 0.020*\"said\" + 0.012*\"red\" + 0.010*\"like\" + 0.010*\"going\" + 0.010*\"need\" + 0.010*\"bit\" + 0.010*\"guy\" + 0.010*\"little\"\n",
      "INFO : topic #2 (0.050): 0.002*\"food\" + 0.001*\"fiftyforward\" + 0.001*\"people\" + 0.001*\"says\" + 0.001*\"bordeaux\" + 0.001*\"day\" + 0.001*\"members\" + 0.001*\"center\" + 0.001*\"meals\" + 0.001*\"seniors\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : topic diff=1.069302, rho=0.353553\n",
      "INFO : -6.599 per-word bound, 97.0 perplexity estimate based on a held-out corpus of 106 documents with 64576 words\n",
      "INFO : PROGRESS: pass 7, at document #106/106\n",
      "INFO : topic #19 (0.050): 0.044*\"hong\" + 0.042*\"kong\" + 0.015*\"people\" + 0.015*\"independence\" + 0.013*\"says\" + 0.013*\"beijing\" + 0.010*\"china\" + 0.010*\"city\" + 0.008*\"protests\" + 0.008*\"time\"\n",
      "INFO : topic #3 (0.050): 0.020*\"says\" + 0.015*\"office\" + 0.010*\"work\" + 0.010*\"employees\" + 0.009*\"people\" + 0.009*\"pandemic\" + 0.008*\"care\" + 0.008*\"boston\" + 0.007*\"percent\" + 0.007*\"business\"\n",
      "INFO : topic #6 (0.050): 0.022*\"dress\" + 0.022*\"midsommar\" + 0.013*\"went\" + 0.013*\"queen\" + 0.013*\"auction\" + 0.013*\"academy\" + 0.013*\"worn\" + 0.013*\"museum\" + 0.013*\"sold\" + 0.013*\"items\"\n",
      "INFO : topic #16 (0.050): 0.025*\"says\" + 0.011*\"people\" + 0.011*\"boston\" + 0.009*\"services\" + 0.008*\"home\" + 0.008*\"technology\" + 0.006*\"company\" + 0.006*\"care\" + 0.006*\"work\" + 0.006*\"software\"\n",
      "INFO : topic #7 (0.050): 0.054*\"hong\" + 0.048*\"kong\" + 0.025*\"said\" + 0.011*\"government\" + 0.010*\"china\" + 0.009*\"mainland\" + 0.008*\"immigration\" + 0.008*\"law\" + 0.007*\"kongs\" + 0.007*\"citys\"\n",
      "INFO : topic diff=0.822953, rho=0.333333\n",
      "INFO : -6.572 per-word bound, 95.1 perplexity estimate based on a held-out corpus of 106 documents with 64576 words\n",
      "INFO : PROGRESS: pass 8, at document #106/106\n",
      "INFO : topic #19 (0.050): 0.045*\"hong\" + 0.042*\"kong\" + 0.015*\"people\" + 0.015*\"independence\" + 0.013*\"says\" + 0.013*\"beijing\" + 0.010*\"china\" + 0.010*\"city\" + 0.008*\"protests\" + 0.008*\"time\"\n",
      "INFO : topic #14 (0.050): 0.017*\"roy\" + 0.017*\"fiftyforward\" + 0.015*\"food\" + 0.015*\"life\" + 0.011*\"foundation\" + 0.011*\"bordeaux\" + 0.009*\"said\" + 0.009*\"years\" + 0.009*\"hot\" + 0.009*\"members\"\n",
      "INFO : topic #9 (0.050): 0.026*\"company\" + 0.023*\"says\" + 0.022*\"cohen\" + 0.017*\"mallett\" + 0.012*\"people\" + 0.012*\"ezcater\" + 0.011*\"day\" + 0.011*\"layoffs\" + 0.011*\"zoom\" + 0.009*\"work\"\n",
      "INFO : topic #17 (0.050): 0.012*\"hotel\" + 0.008*\"year\" + 0.008*\"travel\" + 0.008*\"ll\" + 0.008*\"caribbean\" + 0.008*\"night\" + 0.006*\"covid\" + 0.006*\"like\" + 0.006*\"new\" + 0.006*\"stay\"\n",
      "INFO : topic #11 (0.050): 0.091*\"li\" + 0.044*\"school\" + 0.024*\"nashville\" + 0.017*\"elementary\" + 0.017*\"middle\" + 0.015*\"tn\" + 0.013*\"food\" + 0.013*\"ul\" + 0.013*\"drive\" + 0.012*\"bus\"\n",
      "INFO : topic diff=0.628081, rho=0.316228\n",
      "INFO : -6.554 per-word bound, 94.0 perplexity estimate based on a held-out corpus of 106 documents with 64576 words\n",
      "INFO : PROGRESS: pass 9, at document #106/106\n",
      "INFO : topic #12 (0.050): 0.001*\"li\" + 0.001*\"people\" + 0.001*\"school\" + 0.001*\"says\" + 0.001*\"testing\" + 0.001*\"said\" + 0.001*\"new\" + 0.000*\"work\" + 0.000*\"coronavirus\" + 0.000*\"qanon\"\n",
      "INFO : topic #13 (0.050): 0.008*\"testing\" + 0.005*\"mail\" + 0.005*\"new\" + 0.005*\"obama\" + 0.005*\"qanon\" + 0.005*\"people\" + 0.005*\"week\" + 0.005*\"mr\" + 0.004*\"state\" + 0.004*\"trump\"\n",
      "INFO : topic #7 (0.050): 0.054*\"hong\" + 0.048*\"kong\" + 0.025*\"said\" + 0.011*\"government\" + 0.010*\"china\" + 0.009*\"mainland\" + 0.008*\"immigration\" + 0.008*\"law\" + 0.007*\"kongs\" + 0.007*\"citys\"\n",
      "INFO : topic #11 (0.050): 0.091*\"li\" + 0.044*\"school\" + 0.024*\"nashville\" + 0.017*\"elementary\" + 0.017*\"middle\" + 0.015*\"tn\" + 0.013*\"food\" + 0.013*\"ul\" + 0.013*\"drive\" + 0.012*\"bus\"\n",
      "INFO : topic #9 (0.050): 0.026*\"company\" + 0.023*\"says\" + 0.022*\"cohen\" + 0.017*\"mallett\" + 0.012*\"people\" + 0.012*\"ezcater\" + 0.011*\"day\" + 0.011*\"layoffs\" + 0.011*\"zoom\" + 0.009*\"work\"\n",
      "INFO : topic diff=0.477664, rho=0.301511\n"
     ]
    }
   ],
   "source": [
    "lda_model = gensim.models.ldamodel.LdaModel(news_corpus, num_topics=20, id2word=id2word, passes=10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : storing corpus in Matrix Market format to ./gensim_files/news_corpus.mm\n",
      "INFO : saving sparse matrix to ./gensim_files/news_corpus.mm\n",
      "INFO : PROGRESS: saving document #0\n",
      "INFO : saved 106x4236 matrix, density=8.715% (39131/449016)\n",
      "INFO : saving MmCorpus index to ./gensim_files/news_corpus.mm.index\n",
      "INFO : saving Dictionary object under ./gensim_files/news_dictionary, separately None\n",
      "INFO : saved ./gensim_files/news_dictionary\n",
      "INFO : saving LdaState object under ./gensim_files/lda_news_corpus_10iters.model.state, separately None\n",
      "INFO : saved ./gensim_files/lda_news_corpus_10iters.model.state\n",
      "INFO : saving LdaModel object under ./gensim_files/lda_news_corpus_10iters.model, separately ['expElogbeta', 'sstats']\n",
      "INFO : storing np array 'expElogbeta' to ./gensim_files/lda_news_corpus_10iters.model.expElogbeta.npy\n",
      "INFO : not storing attribute dispatcher\n",
      "INFO : not storing attribute id2word\n",
      "INFO : not storing attribute state\n",
      "INFO : saved ./gensim_files/lda_news_corpus_10iters.model\n"
     ]
    }
   ],
   "source": [
    "# how to store corpus to disk\n",
    "from gensim.corpora import MmCorpus\n",
    "if not os.path.isdir(\"./gensim_files\"):\n",
    "    os.mkdir(\"./gensim_files\")\n",
    "MmCorpus.serialize('./gensim_files/news_corpus.mm', news_corpus)\n",
    "\n",
    "# how to store dictionary to disk\n",
    "id2word_ccp.save('./gensim_files/news_dictionary')\n",
    "\n",
    "# how to store model to disk \n",
    "lda_model.save('./gensim_files/lda_news_corpus_10iters.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : loaded corpus index from ./gensim_files/news_corpus.mm.index\n",
      "INFO : initializing cython corpus reader from ./gensim_files/news_corpus.mm\n",
      "INFO : accepted corpus with 106 documents, 4236 features, 39131 non-zero entries\n"
     ]
    }
   ],
   "source": [
    "outputFileName = 'news_covid_lda_vis.html'\n",
    "news_corpus = MmCorpus('./gensim_files/news_corpus.mm')\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, news_corpus, id2word)\n",
    "pyLDAvis.prepared_data_to_html(vis)\n",
    "pyLDAvis.save_html(vis,outputFileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.corpora.dictionary.Dictionary at 0x7f7fa505a710>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
